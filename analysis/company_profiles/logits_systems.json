{
  "company": "logits.systems",
  "dominant_topics": [
    1,
    3,
    -1,
    0
  ],
  "topic_counts": {
    "1": 2,
    "3": 1,
    "-1": 1,
    "0": 1
  },
  "topic_descriptions": {
    "1": "face, recognition, biometric, attendance, fingerprint",
    "3": "vantage, development, document, ai, prioxis",
    "0": "access, information, security, data, piam"
  },
  "framing_scores": {
    "framing_ethical": 0.8823529411764706,
    "framing_surveillance": 0.1176470588235294,
    "framing_market": 0.0,
    "num_docs": 5
  },
  "topic_timeline": {
    "20": {
      "-1": 1,
      "0": 1,
      "1": 2,
      "3": 1
    }
  },
  "sample_docs": [
    "What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration. What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration. What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration.",
    "The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future.",
    "Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Collection / Storage, Ingestion, Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers. Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Collection / Storage, Ingestion, Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers. Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers.",
    "Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. Saket Deshmukh Co-founder I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind Deore Co-founder Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim Roy Advisor Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses. Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. Saket Deshmukh Co-founder I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind Deore Co-founder Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim Roy Advisor Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses. Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses.",
    "With GDPR and CCPA, enforced cloud companies needs to comply data for fair usage. This is brief framework to get started … For all those data science developer and companies who are are under the ambit of compliance, are the potential consumer of this framework. Risk = Asset (individual's data) + Vulnerability + Threat Therefore, we need risk management, assessment and treatment with organisation defined 'controls'. Formal definitions: Risk Management : The coordinated activities to direct and control an organisation with regards to risk. Risk Assessment : The overall process of risk identification, risk analysis and risk evaluation. Risk Treatment : The process to modify risk, using defined controls. Let us consider how we capture natural person's information. A practical approach to risk management for such dataset would be, a three step process: Step 1 : 1. Asses Risk — Data must NOT be: Inaccurate, Insufficient or Out-Of-Date data, Kept for too long (dormant account), Excessive or Irrelevant information, Disclosed to the wrong people, Insecurely transmitted or stored, Used in ways that are unacceptable or unexpected. 2. Identify Risk — Likelihood and level of severity must be set as Low, Medium and High. FIG-2 : Risk and countermeasures FIG-3 : Assess the costs and value Step 2: 1. Classify Data — Data must have sufficient meta information and labeling, eg. : personal data, minor data, gender, special category like high-net-worth individuals, etc. 2. Take Action Control — The kind of treatment an organisation may deploy to take effective control over its dataset are: Reduce collected data, Pseudonyme information, Retention policy, Secure destruction of information, and purge mechanism, Access control, Training and awareness, Contracts or data-sharing agreements with 3rd party of Processor, Acceptable use policy in the SDK or APP, Subject access request process, 3. Implement incident, management response - Report the data breach to the supervising authority. This needs to be formally structured. Step 3: Demonstrate ongoing risk and incident monitoring. Data protection is not a destiny, it's a journey and hence embedding data protection should be the end goal. People, Process and Technology can enable it. Scenario Let us consider we have a facial images dataset of natural persons and we would like to protect it: Images of a natural person is biometric information, and it is considered to be sensitive data under Privacy Impact Assessment (PIA). Biometric verification must be consent based and hence the natural person MUST be aware of, what is it being used for. The adapted technology should make their lives more secure and easy and not the other way around. With Anonymization, facial images coming from natural persons will be dis-associated from their identity and this is an irreversible process. But this process complies with the standards and such data can be utilized to make the models more robust. After all, real world facial images are hard to collect. On the other hand, Pseudonymization is a process whereby a data cannot be attributed to a natural person without the use of additional information, but it will be considered Pseudonymized when this additional information (secret) is kept separately in a strongly secured place. This additional information is called tokenization. Such a technique is useful to store user biometric data for future validation. To further mitigate the risk, natural persons facial images shall be converted to embeddings (e.g.: 128, 152 bytes long) and stored rather than raw images, this is done using facial AI models. The advantage here is that you can keep the user identity intact using Pseudonymization, but embedding data would be of no use unless the model that created it is unknown to the unauthorised person. Please leave a comment if you feel we can better this framework and yet keep it simple. With GDPR and CCPA, enforced cloud companies needs to comply data for fair usage. This is brief framework to get started … For all those data science developer and companies who are are under the ambit of compliance, are the potential consumer of this framework. Risk = Asset (individual's data) + Vulnerability + Threat Therefore, we need risk management, assessment and treatment with organisation defined 'controls'. Formal definitions: Risk Management : The coordinated activities to direct and control an organisation with regards to risk. Risk Assessment : The overall process of risk identification, risk analysis and risk evaluation. Risk Treatment : The process to modify risk, using defined controls. Let us consider how we capture natural person's information. A practical approach to risk management for such dataset would be, a three step process: Step 1 : 1. Asses Risk — Data must NOT be: Inaccurate, Insufficient or Out-Of-Date data, Kept for too long (dormant account), Excessive or Irrelevant information, Disclosed to the wrong people, Insecurely transmitted or stored, Used in ways that are unacceptable or unexpected. 2. Identify Risk — Likelihood and level of severity must be set as Low, Medium and High. FIG-2 : Risk and countermeasures FIG-3 : Assess the costs and value Step 2: 1. Classify Data — Data must have sufficient meta information and labeling, eg. : personal data, minor data, gender, special category like high-net-worth individuals, etc. 2. Take Action Control — The kind of treatment an organisation may deploy to take effective control over its dataset are: Reduce collected data, Pseudonyme information, Retention policy, Secure destruction of information, and purge mechanism, Access control, Training and awareness, Contracts or data-sharing agreements with 3rd party of Processor, Acceptable use policy in the SDK or APP, Subject access request process, 3. Implement incident, management response - Report the data breach to the supervising authority. This needs to be formally structured. Step 3: Demonstrate ongoing risk and incident monitoring. Data protection is not a destiny, it's a journey and hence embedding data protection should be the end goal. People, Process and Technology can enable it. Scenario Let us consider we have a facial images dataset of natural persons and we would like to protect it: Images of a natural person is biometric information, and it is considered to be sensitive data under Privacy Impact Assessment (PIA). Biometric verification must be consent based and hence the natural person MUST be aware of, what is it being used for. The adapted technology should make their lives more secure and easy and not the other way around. With Anonymization, facial images coming from natural persons will be dis-associated from their identity and this is an irreversible process. But this process complies with the standards and such data can be utilized to make the models more robust. After all, real world facial images are hard to collect. On the other hand, Pseudonymization is a process whereby a data cannot be attributed to a natural person without the use of additional information, but it will be considered Pseudonymized when this additional information (secret) is kept separately in a strongly secured place. This additional information is called tokenization. Such a technique is useful to store user biometric data for future validation. To further mitigate the risk, natural persons facial images shall be converted to embeddings (e.g.: 128, 152 bytes long) and stored rather than raw images, this is done using facial AI models. The advantage here is that you can keep the user identity intact using Pseudonymization, but embedding data would be of no use unless the model that created it is unknown to the unauthorised person. Please leave a comment if you feel we can better this framework and yet keep it simple. With GDPR and CCPA, enforced cloud companies needs to comply data for fair usage. This is brief framework to get started … For all those data science developer and companies who are are under the ambit of compliance, are the potential consumer of this framework. Risk = Asset (individual's data) + Vulnerability + Threat Therefore, we need risk management, assessment and treatment with organisation defined 'controls'. Risk Management : The coordinated activities to direct and control an organisation with regards to risk. Risk Assessment : The overall process of risk identification, risk analysis and risk evaluation. Risk Treatment : The process to modify risk, using defined controls. Let us consider how we capture natural person's information. A practical approach to risk management for such dataset would be, a three step process: 1. Asses Risk — Data must NOT be: Inaccurate, Insufficient or Out-Of-Date data, Kept for too long (dormant account), Excessive or Irrelevant information, Disclosed to the wrong people, Insecurely transmitted or stored, Used in ways that are unacceptable or unexpected. 2. Identify Risk — Likelihood and level of severity must be set as Low, Medium and High. FIG-2 : Risk and countermeasures FIG-3 : Assess the costs and value 1. Classify Data — Data must have sufficient meta information and labeling, eg. : personal data, minor data, gender, special category like high-net-worth individuals, etc. 2. Take Action Control — The kind of treatment an organisation may deploy to take effective control over its dataset are: Secure destruction of information, and purge mechanism, Contracts or data-sharing agreements with 3rd party of Processor, Acceptable use policy in the SDK or APP, Subject access request process, 3. Implement incident, management response - Report the data breach to the supervising authority. This needs to be formally structured. Demonstrate ongoing risk and incident monitoring. Data protection is not a destiny, it's a journey and hence embedding data protection should be the end goal. People, Process and Technology can enable it. Let us consider we have a facial images dataset of natural persons and we would like to protect it: Images of a natural person is biometric information, and it is considered to be sensitive data under Privacy Impact Assessment (PIA). Biometric verification must be consent based and hence the natural person MUST be aware of, what is it being used for. The adapted technology should make their lives more secure and easy and not the other way around. With Anonymization, facial images coming from natural persons will be dis-associated from their identity and this is an irreversible process. But this process complies with the standards and such data can be utilized to make the models more robust. After all, real world facial images are hard to collect. On the other hand, Pseudonymization is a process whereby a data cannot be attributed to a natural person without the use of additional information, but it will be considered Pseudonymized when this additional information (secret) is kept separately in a strongly secured place. This additional information is called tokenization. Such a technique is useful to store user biometric data for future validation. To further mitigate the risk, natural persons facial images shall be converted to embeddings (e.g.: 128, 152 bytes long) and stored rather than raw images, this is done using facial AI models. The advantage here is that you can keep the user identity intact using Pseudonymization, but embedding data would be of no use unless the model that created it is unknown to the unauthorised person. Please leave a comment if you feel we can better this framework and yet keep it simple."
  ]
}