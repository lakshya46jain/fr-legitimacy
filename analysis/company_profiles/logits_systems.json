{
  "company": "logits.systems",
  "dominant_topics": [
    0,
    -1
  ],
  "topic_counts": {
    "0": 4,
    "-1": 2
  },
  "topic_descriptions": {
    "0": "access, security, data, time, use"
  },
  "framing_scores": null,
  "topic_timeline": {
    "20": {
      "-1": 2,
      "0": 4
    }
  },
  "sample_docs": [
    "This is an open secret, when any data breach happens, our online credentials and personal information gets into the dark web, this data is then used against us either to break into our accounts or to do a cyber crimes. Out of 4 billion active internet users, 11 billion accounts are breached. With increasing web-services, 80% of data breaches are enabled by weak, reused or stolen credentials. Our credentials and personal data are all over the web and only 5% of companies keep it protected. People are living in vulnerable circumstances with this levels of account security and personal data protection. Enterprises are facing huge losses in data breach due to weak OR compromised credentials used by the employees. Credentials are supposed to be the front gates of the castle but apparently they are breached. Logit solves this 'last mile' problem by offering private and secured way to control your digital footprints on the internet. This is an open secret, when any data breach happens, our online credentials and personal information gets into the dark web, this data is then used against us either to break into our accounts or to do a cyber crimes. Out of 4 billion active internet users, 11 billion accounts are breached. With increasing web-services, 80% of data breaches are enabled by weak, reused or stolen credentials. Our credentials and personal data are all over the web and only 5% of companies keep it protected. People are living in vulnerable circumstances with this levels of account security and personal data protection. Enterprises are facing huge losses in data breach due to weak OR compromised credentials used by the employees. Credentials are supposed to be the front gates of the castle but apparently they are breached. Logit solves this 'last mile' problem by offering private and secured way to control your digital footprints on the internet.",
    "What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration. What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration. What is required is a safe, secured and yet simple biometric solution. Logits system's face authentication brings many advantages over traditional fingerprint or iris biometrics. Logits systems biometric offers fast, highly accurate, easy to use, runs on user devices which means no cloud computing is required and many more. Data protection is a new threat to the modern world. Face biometric with weak liveness can cause intruders to get them in as impostors. We have given strong emphasis on liveness detection along with anomaly algorithms to make it unbreakable, that inherently solves fake user registration.",
    "The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. The Chinese emperor Ts'in, she was already authenticating specific seals with a fingerprint way back in the second century B. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. In 1858 William James Herschel , a British administrator in India used Fingerprints. Having been put in charge of building roads in Bengal, he had his subcontractors sign contracts with their fingers. That was an early form of biometric authentication and a sure way of being able to find them more quickly if they defaulted. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. At the end of the 19th century, Bertillon , a French police officer, took the first steps in scientific policing. He used body measurements taken of specific anatomical characteristics to identify reoffending criminals , a technique that often proved successful. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. French police in Paris (préfecture de police) started to initiate this process in 1888 with its Forensic Identification Unit (mugshot and anthropometry). Four prints were instituted in 1894, and ten prints were added in 1904. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.K., the Metropolitan Police started the use of biometrics for identification in 1901. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. In the U.S., it was initiated by the New York police in 1902 and by the FBI in 1924. The measurement of unique patterns (aka behavioural biometrics) is not new either. It goes back to the 1860s. Telegraph operators using Morse code recognized each other by the way they would send dash and dot signals. During World War II, allied forces used the same method to identify senders and authentication messages they received. This process is the basic principle of biometric systems: to identify a person based on specific characteristics . Biometrics are in use for a long time and still, we will be using biometric identifications more than ever. Biometric technologies are evolving and we all will be amazed by the use cases in future.",
    "Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Collection / Storage, Ingestion, Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers. Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Collection / Storage, Ingestion, Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers. Business analysts understand the business needs and pass the requirements to the data analyst. Data analysts understand the data requirements and determine what data is needed to address a business problem. Data analyst then collects the right data from an internal source, or by curating the data ( eg.: scraping ), or from an external source ( eg.: open datasets, Kaggle, research projects ). The raw data needs to get processed before the machine learning team can use it. This is a critical step and hence 'become one with data' . Usually the data is divided into two categories (at high level): 1. Structured Data : CSV, Tabular form data, Columnar, etc. 2. Unstructured Data : Images, audio, video, text, etc. Based on business needs, data analysts collect appropriate training, validation, and test datasets. The data MUST be similar to the real world data, else there will be high variance in the results. Rachel Thoma's blog How (and why) to create a good validation set , describes it neatly. The data comes from multiple sources and we need to collect it through data pipelines and possibly aggregate it into a database / data-lake. 1. Data Engineering : Preparation / Transformation ( Clean, Shape, Augment, Transform, Feature Extraction, Conform ). 2. Machine Learning : Computation 3. Output : Inference, Presentation Spending time in understanding the data distribution and finding the outliers is pivotal, the human brain is excellent at this job, hence data analysts must inspect the data manually first. Primarily, during cleaning we are trying to find missing labels, incorrect labels, duplicates, incomplete data, missing fields, etc. With my personal experience, data dimensionality reduction is a very important step, it not just gives better data insight but also reduces the model size, that means low compute power, low memory requirement, fast training time and much better results. It's an analyst's job to find how much details actually matter and thereby how many features need to be captured. The model must be initialised with proper random seed, and data must be normalised and standardised for training, the same normalisation and standardised parameters must be used during inference also. Data augmentation is cosmetic change on images, and at times gives better results, it shouldn't be too overwhelming. A data designer is usually worried about how the data is stored, meaning he/she tries to understand the relationships between the incoming data field and design the structure so that it can be stored. This data has to be managed properly with security access such that only authorised people should have access to it, and has to be backed up too. This is the time when data administrators must consider GDPR and CCPA compliance , meaning removing customer's personal information linkage and assigning Artificial ID, this process is called Pseudonymization. Machine learning researchers create various machine / deep learning models or explore variations of existing models that can be used for business purposes. Data scientists experiment with such models and play around with model parameters to find the best fit. These are called hyper parameters: Optimiser, Regularisation, Drop-outs, Batch-size, Learning-rate are some important hyper-parameters that need to be tuned to converge model to best fit on a particular dataset. Andrej Karpathy's blog gives more insights 'A Recipe for Training Neural Networks' . During training, a series of evaluation metrics are monitored, to reduce the loss and let the model to converge. This is an iterative process until data best fit with lowest possible loss. Once the model converges, it is then verified with a test-set, ideally with real world dataset, this is called inference. For deployment, models can be transformed from one framework to another using ONNX. For mobile deployment, it can be converted to .tflite format, which utilises flat-buffers instead of protocol-buffers, for fast load-time on mobiles. At this stage we can also perform quantisation on the frozen model from float to int8 . Quantisation techniques reduce the model size at the cost of slight degradation in accuracy, this is again based on the business needs, how much quantisation is required Vs accuracy compromise. Finally, end-to-end business processes and technology is employed to deploy the model that enables the business to ultimately serve its customers.",
    "Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. Saket Deshmukh Co-founder I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind Deore Co-founder Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim Roy Advisor Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses. Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. Saket Deshmukh Co-founder I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind Deore Co-founder Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim Roy Advisor Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses. Team has five decades of experience together in software development at startups. Extensive experience in Embedded systems, Networking, Healthcare, and Authentication. I am responsible for figuring out what customers want. Prior to Logits, I had my own startup in Mobile App development platform space. Milind has 20 years of work experience: As a platform engineer with companies like Starent Networks and Cisco. As machine learning architect with Mellowain and FICO. Milind believes in the power of communities and therefore active AI/ML/IoT community member and organizer. Conducted various bootcamps and hackathons. Open source contributor, entrepreneur, learner. Milind mentored and won awards for various projects like : Eclipse IoT 2015, TI India challenge 2016, Intel Ultimate coder challenge 2016. Multiple patents in the area of facial authentication. Ashim's technology journey has crisscrossed countries and continents and he brings diversity and depth of 30+ years of experience to his role of co-founder and CEO of Cardiotrack. Ashim did his BE in Electronics and Electrical Engineering from BITS, Pilani; Masters from IIT, Delhi and PhD from the University of Adelaide. He has spent about 24 years in the telecom world most of it in the US and holds several patents. He has been an adjunct professor at Catholic University, Washington DC. Ashim revels in the challenges and highs of the start-up world and is a fierce proponent of technology that benefits the under-served masses."
  ]
}